{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": [
    "### Read file content and split into words\n",
    "**Try reading file content:**"
   ],
   "id": "f3e3ecf6d39a64ef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T17:57:22.828510Z",
     "start_time": "2025-11-07T17:57:22.823111Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(\"sample_text.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "print(\"Total number of characters:\", len(raw_text))\n",
    "print(raw_text[:99])"
   ],
   "id": "19109bd8ea9494b2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Split text on whitespace, comman and period characters:**",
   "id": "9e409096566017e2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T17:57:22.863069Z",
     "start_time": "2025-11-07T17:57:22.855625Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "# remove redundant whitespaces\n",
    "preprocessed = [item for item in preprocessed if item.strip()]\n",
    "\n",
    "print (preprocessed[:30])"
   ],
   "id": "e981fb2332549cbd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Convert tokens into token IDs\n",
    "**Create a vocabulary**"
   ],
   "id": "400f1d84938093b2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T17:57:22.892399Z",
     "start_time": "2025-11-07T17:57:22.887730Z"
    }
   },
   "cell_type": "code",
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "\n",
    "all_tokens.extend([ \"<|endoftext|>\", \"<|unk|>\" ])\n",
    "\n",
    "vacab_size = len(all_tokens)\n",
    "print (vacab_size)"
   ],
   "id": "675068a475a2b19d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1132\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T17:57:22.921211Z",
     "start_time": "2025-11-07T17:57:22.916731Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vocab = { token:integer for integer, token in enumerate(all_tokens) }\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    print (item)\n",
    "    if i >= 20:\n",
    "        break"
   ],
   "id": "de96b741d8788901",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T17:57:22.968873Z",
     "start_time": "2025-11-07T17:57:22.964761Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print (item)"
   ],
   "id": "3b3d9e8de3b5fe15",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('younger', 1127)\n",
      "('your', 1128)\n",
      "('yourself', 1129)\n",
      "('<|endoftext|>', 1130)\n",
      "('<|unk|>', 1131)\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Implement a simple text tokenizer**",
   "id": "bf8da1dba9b9bdb5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T17:57:23.018128Z",
     "start_time": "2025-11-07T17:57:23.012580Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        # create an inverse vocabulry that maps token ids back to the original text tokens\n",
    "        self.int_to_str = { i:s for s, i in vocab.items() }\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        # process input text into token ids\n",
    "        preprocessed = [ item.strip() for item in preprocessed if item.strip() ]\n",
    "        # replace unknown words by <|unk|> tokens\n",
    "        preprocessed = [ item if item in self.str_to_int else \"<|unk|>\" for item in preprocessed ]\n",
    "        ids = [ self.str_to_int[s] for s in preprocessed ]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        # convert token ids back into text\n",
    "        text = \" \".join([ self.int_to_str[i] for i in ids ])\n",
    "        # remove spaces before the specified punctuation\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text\n"
   ],
   "id": "4f8163088675ad60",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Test the tokenizer**",
   "id": "e06b6b9fc635d16"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T17:57:23.036135Z",
     "start_time": "2025-11-07T17:57:23.031818Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "text = \"\"\"\"It's the last he painted, you know,\"\n",
    "       Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print (ids)"
   ],
   "id": "4897e0ef37422051",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T17:57:23.091409Z",
     "start_time": "2025-11-07T17:57:23.088749Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "a70ce19735161e80",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T11:59:07.919536Z",
     "start_time": "2025-10-26T11:59:07.914060Z"
    }
   },
   "cell_type": "markdown",
   "source": "**Turn the ids back to text**",
   "id": "7af4113a8e5b3cbe"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T17:57:23.116144Z",
     "start_time": "2025-11-07T17:57:23.113064Z"
    }
   },
   "cell_type": "code",
   "source": "print (tokenizer.decode(ids))",
   "id": "5fcc1bb4eefbf865",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T17:57:23.165570Z",
     "start_time": "2025-11-07T17:57:23.161548Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "\n",
    "print (tokenizer.encode(text))"
   ],
   "id": "6a2df5dccf3e24fa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T17:57:23.217243Z",
     "start_time": "2025-11-07T17:57:23.214275Z"
    }
   },
   "cell_type": "code",
   "source": "print (tokenizer.decode(tokenizer.encode(text)))",
   "id": "3e1d7d7b28d73869",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Byte pair encoding\n",
   "id": "233d7e0ca3821bf0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T17:57:24.393729Z",
     "start_time": "2025-11-07T17:57:23.260955Z"
    }
   },
   "cell_type": "code",
   "source": "%pip install tiktoken",
   "id": "f6fecce2ef8cc861",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in /Users/kodlan/.venvs/llm_from_scratch/lib/python3.12/site-packages (0.12.0)\r\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/kodlan/.venvs/llm_from_scratch/lib/python3.12/site-packages (from tiktoken) (2025.10.23)\r\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/kodlan/.venvs/llm_from_scratch/lib/python3.12/site-packages (from tiktoken) (2.32.5)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/kodlan/.venvs/llm_from_scratch/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.4.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/kodlan/.venvs/llm_from_scratch/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.11)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/kodlan/.venvs/llm_from_scratch/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2.5.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/kodlan/.venvs/llm_from_scratch/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2025.10.5)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.3\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T17:57:24.440794Z",
     "start_time": "2025-11-07T17:57:24.427402Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from importlib.metadata import version\n",
    "import tiktoken\n",
    "\n",
    "print (\"tiktoken version:\", version(\"tiktoken\"))"
   ],
   "id": "78486b2f16152b2a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.12.0\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T17:57:24.766503Z",
     "start_time": "2025-11-07T17:57:24.455484Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "text = (\"Hello, do you like tea? <|endoftext|> In the sunlit terraces of the someunknownplace.\")\n",
    "\n",
    "integers = tokenizer.encode(text, allowed_special={ \"<|endoftext|>\" })\n",
    "\n",
    "print (integers)"
   ],
   "id": "4b9a23d5ea50b2cb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 286, 262, 617, 34680, 5372, 13]\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Convert token IDs back to text**",
   "id": "5a8ab7b18505677f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T17:57:24.780127Z",
     "start_time": "2025-11-07T17:57:24.776721Z"
    }
   },
   "cell_type": "code",
   "source": [
    "strings = tokenizer.decode(integers)\n",
    "print (strings)"
   ],
   "id": "34984824d6793d76",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the someunknownplace.\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Try the BPE tokenizer from the tiktoken library on the unknown words “Akwirw ier” and\n",
    "print the individual token IDs. Then, call the decode function on each of the resulting\n",
    "integers in this list to reproduce the mapping shown in figure 2.11. Lastly, call the\n",
    "decode method on the token IDs to check whether it can reconstruct the original\n",
    "input, “Akwirw ier.”"
   ],
   "id": "8532cd3dc0f576b8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T17:57:24.822014Z",
     "start_time": "2025-11-07T17:57:24.803398Z"
    }
   },
   "cell_type": "code",
   "source": [
    "integers = tokenizer.encode(\"Akwirw ier\")\n",
    "print (integers)"
   ],
   "id": "f9d4b64f34c86bfa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33901, 86, 343, 86, 220, 959]\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T17:57:24.834319Z",
     "start_time": "2025-11-07T17:57:24.831244Z"
    }
   },
   "cell_type": "code",
   "source": [
    "strings = tokenizer.decode(integers)\n",
    "print (strings)"
   ],
   "id": "dd7fd7269e5e925d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Akwirw ier\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T17:57:24.857868Z",
     "start_time": "2025-11-07T17:57:24.854351Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for token in integers:\n",
    "    print (\"\" + str(token) + \" == \\\"\" + tokenizer.decode([token]) + \"\\\"\")"
   ],
   "id": "8978a441bb3c963a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33901 == \"Ak\"\n",
      "86 == \"w\"\n",
      "343 == \"ir\"\n",
      "86 == \"w\"\n",
      "220 == \" \"\n",
      "959 == \"ier\"\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Byte pair encoding builds its vocabulary by iteratively merging frequent characters into subwords and frequent subwords into words. For example, BPE starts with adding all individual single characters to its vocabulary (“a”,\n",
    " “b,” etc.). In the next stage, it merges character combinations that frequently occur together into subwords. For example, “d” and “e” may be merged into the subword “de,” which is common in many English words like “define”,  “depend,” “made,” and “hidden.” The merges are determined by a frequency cutoff."
   ],
   "id": "8ac54167337b4ef9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Data sampling with a sliding window",
   "id": "98e440918f2f9bef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T17:57:24.888226Z",
     "start_time": "2025-11-07T17:57:24.879724Z"
    }
   },
   "cell_type": "code",
   "source": [
    "enc_text = tokenizer.encode(raw_text)\n",
    "print (len(enc_text))"
   ],
   "id": "3a10c976083c3cc0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T17:57:24.908767Z",
     "start_time": "2025-11-07T17:57:24.905676Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# remove the first 50 tokens from the dataset for demonstration purposes,\n",
    "# as it results in a slightly more interesting text passage in the next steps:\n",
    "enc_sample = enc_text[50:]"
   ],
   "id": "7994d2cdb3244a6e",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "One of the easiest and most intuitive ways to create the input–target pairs for the next-\n",
    "word prediction task is to create two variables, x and y, where x contains the input\n",
    "tokens and y contains the targets, which are the inputs shifted by 1:"
   ],
   "id": "526af130caba9aac"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T17:57:24.934342Z",
     "start_time": "2025-11-07T17:57:24.930294Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# the context size determines how many tokens are included in the input\n",
    "context_size = 4\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size + 1]\n",
    "\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:      {y}\")"
   ],
   "id": "ee290939c2d6ad80",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [290, 4920, 2241, 287]\n",
      "y:      [4920, 2241, 287, 257]\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "By processing the inputs along with the targets, which are the inputs shifted by one\n",
    "position, we can create the next-word prediction tasks (see figure 2.12), as follows:"
   ],
   "id": "63d79f9c817925b3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T17:57:24.961876Z",
     "start_time": "2025-11-07T17:57:24.958311Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(context, \"---->\", desired)"
   ],
   "id": "c88ff49bce03780c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[290] ----> 4920\n",
      "[290, 4920] ----> 2241\n",
      "[290, 4920, 2241] ----> 287\n",
      "[290, 4920, 2241, 287] ----> 257\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Everything left of the arrow (---->) refers to the input an LLM would receive, and\n",
    "the token ID on the right side of the arrow represents the target token ID that the\n",
    "LLM is supposed to predict. Let’s repeat the previous code but convert the token IDs\n",
    "into text:"
   ],
   "id": "25d582ada4bab0d2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T17:57:24.991950Z",
     "start_time": "2025-11-07T17:57:24.986941Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
   ],
   "id": "1d5ad89fb610f861",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and ---->  established\n",
      " and established ---->  himself\n",
      " and established himself ---->  in\n",
      " and established himself in ---->  a\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We’ve now created the input–target pairs that we can use for LLM training.",
   "id": "6c34882d95471324"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Implement efficient data loader that iterates over the input dataset and returns the input and target as PyTorch tensors",
   "id": "a1bc0adf27394cd2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Text sample:\n",
    "\n",
    "-----------\n",
    "\"In the heart of the city stood the old library, a relic from a bygone era ...\"\n",
    "\n",
    "```\n",
    "Tensor containing the inputs (x)\n",
    "-------------------------------\n",
    "x = tensor([\n",
    "      [\"In\",   \"the\",  \"heart\", \"of\"  ],\n",
    "      [\"the\",  \"city\", \"stood\", \"the\" ],\n",
    "      [\"old\",  \"library\", \",\",  \"a\"   ],\n",
    "      ...\n",
    "    ])\n",
    "\n",
    "Tensor containing the targets (y)\n",
    "---------------------------------\n",
    "y = tensor([\n",
    "      [\"the\",  \"heart\", \"of\",   \"the\"   ],   <- next words\n",
    "      [\"city\", \"stood\", \"the\",  \"old\"   ],\n",
    "      [\"library\", \",\",  \"a\",    \"relic\" ],\n",
    "      ...\n",
    "    ])\n",
    "```\n",
    "(Each row in `y` is `x` shifted one position to the left.)"
   ],
   "id": "ad7eb20964e52cf9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T17:57:25.019066Z",
     "start_time": "2025-11-07T17:57:25.015073Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# to test this approach\n",
    "token_len = 1000       # number of tokens in the text\n",
    "block_size = 256 # length of the input\n",
    "stride = 128     # number of tokens that intersects in each block\n",
    "\n",
    "for i in range(0, token_len - block_size, stride):\n",
    "    print(f\"input - {i}:{i + 256}, target - {i + 1}:{i + 256 + 1}\")"
   ],
   "id": "aa98e9594c2fbfe7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input - 0:256, target - 1:257\n",
      "input - 128:384, target - 129:385\n",
      "input - 256:512, target - 257:513\n",
      "input - 384:640, target - 385:641\n",
      "input - 512:768, target - 513:769\n",
      "input - 640:896, target - 641:897\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T17:57:26.096730Z",
     "start_time": "2025-11-07T17:57:25.043191Z"
    }
   },
   "cell_type": "code",
   "source": "%pip install torch numpy",
   "id": "221077367bf27f67",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/kodlan/.venvs/llm_from_scratch/lib/python3.12/site-packages (2.2.2)\r\n",
      "Requirement already satisfied: numpy in /Users/kodlan/.venvs/llm_from_scratch/lib/python3.12/site-packages (2.3.4)\r\n",
      "Requirement already satisfied: filelock in /Users/kodlan/.venvs/llm_from_scratch/lib/python3.12/site-packages (from torch) (3.20.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/kodlan/.venvs/llm_from_scratch/lib/python3.12/site-packages (from torch) (4.15.0)\r\n",
      "Requirement already satisfied: sympy in /Users/kodlan/.venvs/llm_from_scratch/lib/python3.12/site-packages (from torch) (1.14.0)\r\n",
      "Requirement already satisfied: networkx in /Users/kodlan/.venvs/llm_from_scratch/lib/python3.12/site-packages (from torch) (3.5)\r\n",
      "Requirement already satisfied: jinja2 in /Users/kodlan/.venvs/llm_from_scratch/lib/python3.12/site-packages (from torch) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec in /Users/kodlan/.venvs/llm_from_scratch/lib/python3.12/site-packages (from torch) (2025.9.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/kodlan/.venvs/llm_from_scratch/lib/python3.12/site-packages (from jinja2->torch) (3.0.3)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/kodlan/.venvs/llm_from_scratch/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.3\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T17:58:12.366013Z",
     "start_time": "2025-11-07T17:58:12.359534Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, block_size, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt)\n",
    "\n",
    "        # use sliding window to chunk the book into overlapping sequences of block_size\n",
    "        for i in range(0, len(token_ids) - block_size, stride):\n",
    "            input_chunk = token_ids[i:i + block_size]\n",
    "            target_chunk = token_ids[i + 1 : i + block_size + 1]\n",
    "\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    # return the total number of rows in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    # return a single row from the dataset\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n"
   ],
   "id": "a905e0ac7dc7693a",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "A data loader to generate batches with input-with pairs",
   "id": "5ae1519a99bf2539"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T17:58:13.126900Z",
     "start_time": "2025-11-07T17:58:13.123495Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, block_size=256, stride=128,\n",
    "                         shuffle=True, drop_last=True, num_workers=0):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, block_size=block_size, stride=stride)\n",
    "    dataloader = DataLoader(dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=shuffle,\n",
    "                            drop_last=drop_last,\n",
    "                            num_workers=num_workers)\n",
    "    return dataloader"
   ],
   "id": "5a40636696f9b64d",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T15:28:10.728142Z",
     "start_time": "2025-11-07T15:28:10.725920Z"
    }
   },
   "cell_type": "markdown",
   "source": "Let's test the dataloader",
   "id": "fe2085900d08267a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T17:59:59.008097Z",
     "start_time": "2025-11-07T17:59:58.938362Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataloader = create_dataloader_v1(raw_text, batch_size=1, block_size=4, stride=1, shuffle=False)\n",
    "data_iter = iter(dataloader)\n",
    "\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)\n",
    "\n",
    "second_batch = next(data_iter)\n",
    "print(second_batch)"
   ],
   "id": "7f07ac528d21ac27",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n",
      "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T18:02:01.768123Z",
     "start_time": "2025-11-07T18:02:01.765863Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "4f0c45e663d0efe9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T18:03:55.875664Z",
     "start_time": "2025-11-07T18:03:55.842720Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataloader = create_dataloader_v1(raw_text, batch_size=8, block_size=4, stride=4, shuffle=False)\n",
    "data_iter = iter(dataloader)\n",
    "\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)\n",
    "\n"
   ],
   "id": "7e2644203a9de7fb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Targets:\n",
      " tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   922],\n",
      "        [ 5891,  1576,   438,   568],\n",
      "        [  340,   373,   645,  1049],\n",
      "        [ 5975,   284,   502,   284],\n",
      "        [ 3285,   326,    11,   287]])\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c18c3ffb19a72d5b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
