{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": "## Coding attention mechanisms\n",
   "id": "42851f868bbbf013"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Implement four different variants of the attention mechanisms:\n",
    "* Simlified self-attention\n",
    "* Self-attention\n",
    "* Casual attention\n",
    "* Multi-head attention"
   ],
   "id": "c36a85ba90a30683"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## A simple self-attention mechanism without trainable weights\n",
    "\n",
    "The goal of self-attention is to compute a context vector for each input element that combines information from all other input elements. A context vector can be interpreted as an enriched embedding vector, which is created by incorporating information from all other elements in the sequense.\n",
    "\n",
    "Consider following input sentance, which has already been embedded into three-dimentional vectors:\n"
   ],
   "id": "ca2bed2c53ed613a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "%pip install torch numpy",
   "id": "f968e51ba2bb05b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T12:59:20.632860Z",
     "start_time": "2025-12-25T12:59:20.628381Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "inputs = torch.tensor(\n",
    "    [[0.43, 0.15, 0.89],  # your     (x1)\n",
    "     [0.55, 0.87, 0.66],  # journey  (x2)\n",
    "     [0.57, 0.85, 0.64],  # starts   (x3)\n",
    "     [0.22, 0.58, 0.33],  # with     (x4)\n",
    "     [0.77, 0.25, 0.10],  # one      (x5)\n",
    "     [0.05, 0.80, 0.55]]  # step     (x6)\n",
    ")"
   ],
   "id": "e0ab7e47da766a24",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Step1**. In order to calculate the intermediate attention scores for second input element we compute the dot product of x2 with every other input token:",
   "id": "7ec98a14bd44816f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T17:24:30.853803Z",
     "start_time": "2025-11-08T17:24:30.842111Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# the second input token serves as the query\n",
    "query = inputs[1]\n",
    "\n",
    "attn_scores_for_2 = torch.empty(inputs.shape[0])\n",
    "\n",
    "for i, x_i in enumerate(inputs):\n",
    "    attn_scores_for_2 [i] = torch.dot(x_i, query)\n",
    "\n",
    "print(attn_scores_for_2)"
   ],
   "id": "b725e98a4247d215",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The dot product is a measure of similarity because it quantifies how closely two vectors are aligned: a higher dot procuct indicates a greater degree of alignment or similarity between the vectors. In the context of the self-attention mechanisms, the dot product determines the extent to which each element in the sequnce focuses on, or **\"attends to\"** any other element: the higher the dot product, the higher the similarity and attention score between two elements.",
   "id": "b60c917b754358fd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Step2**. Normalize each of the attention scores:\n",
   "id": "71fbae05046a29b7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T17:41:13.703755Z",
     "start_time": "2025-11-08T17:41:13.695589Z"
    }
   },
   "cell_type": "code",
   "source": [
    "attn_weights_2_tmp = attn_scores_for_2 / attn_scores_for_2.sum()\n",
    "\n",
    "print(\"Attention weights: \", attn_weights_2_tmp)\n",
    "print(\"Sum: \", attn_weights_2_tmp.sum())"
   ],
   "id": "a1126868872b2a6c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights:  tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "Sum:  tensor(1.0000)\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**In practice, it's more common to use softmax function for normalization**",
   "id": "1fc680f8968f6764"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T17:51:30.830314Z",
     "start_time": "2025-11-08T17:51:30.825625Z"
    }
   },
   "cell_type": "code",
   "source": [
    "attn_weights_2_softmax = torch.softmax(attn_scores_for_2, dim=0)\n",
    "\n",
    "print(\"Attention weights: \", attn_weights_2_softmax)\n",
    "print(\"Sum: \", attn_weights_2_softmax.sum())"
   ],
   "id": "e45eeb9730cf80ae",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights:  tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum:  tensor(1.)\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "  **Step3**. Calculate the context vector by multiplying the embedded input tokens, with the corresponding attention weights and then summing the resulting vectors. Context vector is the weighted sum of all input vectors, obtained by multiplying each input vector by its corresponding attention weight.",
   "id": "f5d7ddc8e0384250"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T18:37:06.168989Z",
     "start_time": "2025-11-08T18:37:06.163862Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query = inputs[1] # x2\n",
    "\n",
    "print(query.shape)\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "\n",
    "for i, x_i in enumerate(inputs):\n",
    "    context_vec_2 += attn_weights_2_softmax[i] * x_i\n",
    "\n",
    "print(context_vec_2)"
   ],
   "id": "24d67d5014950b24",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3])\n",
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "be0d02788da96269"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "So again all the calculations done manually:\n",
    "```\n",
    "attn_scores_for_2 = input * x2 =\n",
    "    [0.43, 0.15, 0.89]\n",
    "    [0.55, 0.87, 0.66] (x2)\n",
    "    [0.57, 0.85, 0.64]\n",
    "    [0.22, 0.58, 0.33]\n",
    "    [0.77, 0.25, 0.10]\n",
    "    [0.05, 0.80, 0.55]         *   [0.55, 0.87, 0.66] (x2)  =\n",
    "                                   [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865]\n",
    "\n",
    "attn_weights_2_softmax = softmax([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865]) =\n",
    "                                 [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581]\n",
    "\n",
    "context_vec_2 = sum(input[i] * attn_weights_2_softmax[i]) =\n",
    "              = sum(\n",
    "                    [0.43, 0.15, 0.89] * 0.1385,\n",
    "                    [0.55, 0.87, 0.66] * 0.2379\n",
    "                    [0.57, 0.85, 0.64] * 0.2333\n",
    "                    [0.22, 0.58, 0.33] * 0.1240\n",
    "                    [0.77, 0.25, 0.10] * 0.1082\n",
    "                    [0.05, 0.80, 0.55] * 0.1581)\n",
    "              = sum(\n",
    "                    [0.0596, 0.0208, 0.1233]\n",
    "                    [0.1308, 0.2070, 0.1570]\n",
    "                    [0.1330, 0.1983, 0.1493]\n",
    "                    [0.0273, 0.0719, 0.0409]\n",
    "                    [0.0833, 0.0270, 0.0108]\n",
    "                    [0.0079, 0.1265, 0.0870])   =   [0.4419, 0.6515, 0.5683]\n",
    "```\n"
   ],
   "id": "2c1302ec5c1766d0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T18:38:45.790902Z",
     "start_time": "2025-11-08T18:38:45.788558Z"
    }
   },
   "cell_type": "markdown",
   "source": [
    "That was the attention weight for input x2.\n",
    "\n",
    "### Computing attention weightsd for all input tokens"
   ],
   "id": "f3b5793127f444fc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T13:49:52.049335Z",
     "start_time": "2025-12-25T13:49:52.043307Z"
    }
   },
   "cell_type": "code",
   "source": [
    "attn_scores = torch.empty(6, 6)\n",
    "\n",
    "# for i, x_i in enumerate(inputs):\n",
    "#    for j, x_j in enumerate(inputs):\n",
    "#        attn_scores[i, j] = torch.dot(x_i, x_j)\n",
    "\n",
    "# compute attention score tensor (equivalent to the for loop above)\n",
    "attn_scores = inputs @ inputs.T\n",
    "\n",
    "# normalize\n",
    "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "\n",
    "print(attn_scores)"
   ],
   "id": "e50bfb2549a5bd1e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "setting `dim=-1` we apply normalization along the last dimension of the `attn_scores` tensor. If `attn_scores` is a two-dimentional tensor ([rows, columns]) it will normalize across the colums so that the values in each row sum up to 1.\n",
    "\n",
    "Now use attention weights to compute all context vectors via matrix mul:"
   ],
   "id": "297f3581cad2cba4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T13:49:55.121885Z",
     "start_time": "2025-12-25T13:49:55.116615Z"
    }
   },
   "cell_type": "code",
   "source": [
    "all_context_vecs = attn_weights @ inputs\n",
    "print(all_context_vecs)"
   ],
   "id": "b9ac0626f65eac0f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Self-attention with trainable weights (scaled dot-product attention)\n",
    "\n",
    "Let's introduce three trainable weight matrices - `Wq`, `Wk`, `Wv`.\n",
    "Used to project the embedded input tokens x(i) into query, key and value vectors.\n",
    "\n",
    "Let's calculate all the context vectors for input x_2:"
   ],
   "id": "1ae940acca785821"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T14:32:32.045313Z",
     "start_time": "2025-12-25T14:32:32.036629Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x_2 = inputs[1]         # second input element\n",
    "d_in = inputs.shape[1]  # the input embedding size = 3\n",
    "d_out = 2               # the output embedding size\n",
    "# in GPT-like models the input and output dim are usually the same\n",
    "\n",
    "# init three weight matrices\n",
    "torch.manual_seed(123)\n",
    "\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key   = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)  # setting false to reduce clutter in the outputs\n",
    "# TODO: set to True to update these matrices during model training\n",
    "\n",
    "# compute the vectors\n",
    "query_2 = x_2 @ W_query\n",
    "key_2   = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value\n",
    "\n",
    "print(query_2)\n",
    "print(key_2)\n",
    "print(value_2)"
   ],
   "id": "180b45213db6e59a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4306, 1.4551])\n",
      "tensor([0.4433, 1.1419])\n",
      "tensor([0.3951, 1.0037])\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Even though we're computing context vector for x_2, we still need the key and value vectors for all the input elements as they are involved in computing the attention weights with respect to the query q_2",
   "id": "e21eaea849a8efe1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T14:36:56.221023Z",
     "start_time": "2025-12-25T14:36:56.217768Z"
    }
   },
   "cell_type": "code",
   "source": [
    "keys = inputs @ W_key\n",
    "values = inputs @ W_value"
   ],
   "id": "44b72643231bda2e",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's compute the attention score",
   "id": "3ab43fb02367601f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T14:36:56.762898Z",
     "start_time": "2025-12-25T14:36:56.758889Z"
    }
   },
   "cell_type": "code",
   "source": [
    "attn_score_22 = query_2 @ key_2\n",
    "print(attn_score_22)"
   ],
   "id": "4ebfd4531eb3854d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8524)\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "or compute for all attention scores:",
   "id": "7557502e73ab4336"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T14:36:57.151504Z",
     "start_time": "2025-12-25T14:36:57.146114Z"
    }
   },
   "cell_type": "code",
   "source": [
    "attn_scores_2 = query_2 @ keys.T\n",
    "print(attn_scores_2)"
   ],
   "id": "fac3a0e9a16257c6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now compute attention weights from attention scores, by scaling the attention scores and using softmax. However, now we scale by dividing by square root of the embedding dimentions of the keys:",
   "id": "18ca17730b958f87"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T14:36:57.497921Z",
     "start_time": "2025-12-25T14:36:57.492707Z"
    }
   },
   "cell_type": "code",
   "source": [
    "d_k = keys.shape[-1]\n",
    "attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)\n",
    "print(attn_weights_2)"
   ],
   "id": "bdd00f1cfee12707",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**The reason for the normalization by the embedding dimention size is to improve the training performance by avoiding small gradients. This is why this self-attention mechanism is called scaled-dot product attention**\n",
    "\n",
    "The last step is multiplying each value vector with its respective attention weight and them summing them to obtain the context vector."
   ],
   "id": "1bad281bb32ad6c8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T14:36:58.051076Z",
     "start_time": "2025-12-25T14:36:58.047159Z"
    }
   },
   "cell_type": "code",
   "source": [
    "context_vec_2 = attn_weights_2 @ values\n",
    "print(context_vec_2)"
   ],
   "id": "d0bbb38ddee060fe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3061, 0.8210])\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**A query** is analogous to a search query in a database. It represents the current item\n",
    "(e.g., a word or token in a sentence) the model focuses on or tries to understand.\n",
    "The query is used to probe the other parts of the input sequence to determine how\n",
    "much attention to pay to them.\n",
    "\n",
    "**The key** is like a database key used for indexing and searching. In the attention mech-\n",
    "anism, each item in the input sequence (e.g., each word in a sentence) has an associated key. These keys are used to match the query.\n",
    "\n",
    "**The value** in this context is similar to the value in a key-value pair in a database. It\n",
    "represents the actual content or representation of the input items. Once the model\n",
    "determines which keys (and thus which parts of the input) are most relevant to the\n",
    "query (the current focus item), it retrieves the corresponding values."
   ],
   "id": "629fdbda78e32fca"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Self-attention class",
   "id": "d8f1b31e9fa40e3c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T13:07:43.737618Z",
     "start_time": "2025-12-26T13:07:43.732026Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key   = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = x @ self.W_key\n",
    "        queries = x @ self.W_query\n",
    "        values = x @ self.W_value\n",
    "\n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        context_vec = attn_weights @ values\n",
    "\n",
    "        return context_vec"
   ],
   "id": "d0469104065a5f41",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-26T13:09:01.028516Z",
     "start_time": "2025-12-26T13:09:01.023396Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# test\n",
    "torch.manual_seed(123)\n",
    "sa = SelfAttention(d_in, d_out)\n",
    "print(sa(inputs))"
   ],
   "id": "abd37e24ae5515d9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c8fd3e7288a25c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
