{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": "## Coding attention mechanisms\n",
   "id": "42851f868bbbf013"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Implement four different variants of the attention mechanisms:\n",
    "* Simlified self-attention\n",
    "* Self-attention\n",
    "* Casual attention\n",
    "* Multi-head attention"
   ],
   "id": "c36a85ba90a30683"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### A simple self-attention mechanism wihtout trainable weights\n",
    "\n",
    "The goal of self-attention is to compute a context vector for each input element that combines information from all other input elements. A context vector can be interpreted as an enriched embedding vector, which is created by incorporating information from all other elements in the sequense.\n",
    "\n",
    "Consider following input sentance, which has already been embedded into three-dimentional vectors:\n"
   ],
   "id": "ca2bed2c53ed613a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T17:08:13.371650Z",
     "start_time": "2025-11-08T17:08:11.603291Z"
    }
   },
   "cell_type": "code",
   "source": "%pip install torch numpy",
   "id": "f968e51ba2bb05b6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/kodlan/.venvs/llm_from_scratch/lib/python3.12/site-packages (2.2.2)\r\n",
      "Requirement already satisfied: numpy in /Users/kodlan/.venvs/llm_from_scratch/lib/python3.12/site-packages (2.3.4)\r\n",
      "Requirement already satisfied: filelock in /Users/kodlan/.venvs/llm_from_scratch/lib/python3.12/site-packages (from torch) (3.20.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/kodlan/.venvs/llm_from_scratch/lib/python3.12/site-packages (from torch) (4.15.0)\r\n",
      "Requirement already satisfied: sympy in /Users/kodlan/.venvs/llm_from_scratch/lib/python3.12/site-packages (from torch) (1.14.0)\r\n",
      "Requirement already satisfied: networkx in /Users/kodlan/.venvs/llm_from_scratch/lib/python3.12/site-packages (from torch) (3.5)\r\n",
      "Requirement already satisfied: jinja2 in /Users/kodlan/.venvs/llm_from_scratch/lib/python3.12/site-packages (from torch) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec in /Users/kodlan/.venvs/llm_from_scratch/lib/python3.12/site-packages (from torch) (2025.9.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/kodlan/.venvs/llm_from_scratch/lib/python3.12/site-packages (from jinja2->torch) (3.0.3)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/kodlan/.venvs/llm_from_scratch/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.3\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T17:08:18.399Z",
     "start_time": "2025-11-08T17:08:18.393804Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "inputs = torch.tensor(\n",
    "    [[0.43, 0.15, 0.89],  # your     (x1)\n",
    "     [0.55, 0.87, 0.66],  # journey  (x2)\n",
    "     [0.57, 0.85, 0.64],  # starts   (x3)\n",
    "     [0.22, 0.58, 0.33],  # with     (x4)\n",
    "     [0.77, 0.25, 0.10],  # one      (x5)\n",
    "     [0.05, 0.80, 0.55]]  # step     (x6)\n",
    ")"
   ],
   "id": "e0ab7e47da766a24",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Step1**. In order to calculate the intermediate attention scores for second input element we compute the dot product of x2 with every other input token:",
   "id": "7ec98a14bd44816f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T17:24:30.853803Z",
     "start_time": "2025-11-08T17:24:30.842111Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# the second input token serves as the query\n",
    "query = inputs[1]\n",
    "\n",
    "attn_scores_for_2 = torch.empty(inputs.shape[0])\n",
    "\n",
    "for i, x_i in enumerate(inputs):\n",
    "    attn_scores_for_2 [i] = torch.dot(x_i, query)\n",
    "\n",
    "print(attn_scores_for_2)"
   ],
   "id": "b725e98a4247d215",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The dot product is a measure of similarity because it quantifies how closely two vectors are aligned: a higher dot procuct indicates a greater degree of alignment or similarity between the vectors. In the context of the self-attention mechanisms, the dot product determines the extent to which each element in the sequnce focuses on, or **\"attends to\"** any other element: the higher the dot product, the higher the similarity and attention score between two elements.",
   "id": "b60c917b754358fd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Step2**. Normalize each of the attention scores:\n",
   "id": "71fbae05046a29b7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T17:41:13.703755Z",
     "start_time": "2025-11-08T17:41:13.695589Z"
    }
   },
   "cell_type": "code",
   "source": [
    "attn_weights_2_tmp = attn_scores_for_2 / attn_scores_for_2.sum()\n",
    "\n",
    "print(\"Attention weights: \", attn_weights_2_tmp)\n",
    "print(\"Sum: \", attn_weights_2_tmp.sum())"
   ],
   "id": "a1126868872b2a6c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights:  tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "Sum:  tensor(1.0000)\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**In practice, it's more common to use softmax function for normalization**",
   "id": "1fc680f8968f6764"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T17:51:30.830314Z",
     "start_time": "2025-11-08T17:51:30.825625Z"
    }
   },
   "cell_type": "code",
   "source": [
    "attn_weights_2_softmax = torch.softmax(attn_scores_for_2, dim=0)\n",
    "\n",
    "print(\"Attention weights: \", attn_weights_2_softmax)\n",
    "print(\"Sum: \", attn_weights_2_softmax.sum())"
   ],
   "id": "e45eeb9730cf80ae",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights:  tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum:  tensor(1.)\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "  **Step3**. Calculate the context vector by multiplying the embedded input tokens, with the corresponding attention weights and then summing the resulting vectors. Context vector is the weighted sum of all input vectors, obtained by multiplying each input vector by its corresponding attention weight.",
   "id": "f5d7ddc8e0384250"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T18:37:06.168989Z",
     "start_time": "2025-11-08T18:37:06.163862Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query = inputs[1] # x2\n",
    "\n",
    "print(query.shape)\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "\n",
    "for i, x_i in enumerate(inputs):\n",
    "    context_vec_2 += attn_weights_2_softmax[i] * x_i\n",
    "\n",
    "print(context_vec_2)"
   ],
   "id": "24d67d5014950b24",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3])\n",
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "be0d02788da96269"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "So again all the calculations done manually:\n",
    "```\n",
    "attn_scores_for_2 = input * x2 =\n",
    "    [0.43, 0.15, 0.89]\n",
    "    [0.55, 0.87, 0.66] (x2)\n",
    "    [0.57, 0.85, 0.64]\n",
    "    [0.22, 0.58, 0.33]\n",
    "    [0.77, 0.25, 0.10]\n",
    "    [0.05, 0.80, 0.55]         *   [0.55, 0.87, 0.66] (x2)  =\n",
    "                                   [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865]\n",
    "\n",
    "attn_weights_2_softmax = softmax([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865]) =\n",
    "                                 [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581]\n",
    "\n",
    "context_vec_2 = sum(input[i] * attn_weights_2_softmax[i]) =\n",
    "              = sum(\n",
    "                    [0.43, 0.15, 0.89] * 0.1385,\n",
    "                    [0.55, 0.87, 0.66] * 0.2379\n",
    "                    [0.57, 0.85, 0.64] * 0.2333\n",
    "                    [0.22, 0.58, 0.33] * 0.1240\n",
    "                    [0.77, 0.25, 0.10] * 0.1082\n",
    "                    [0.05, 0.80, 0.55] * 0.1581)\n",
    "              = sum(\n",
    "                    [0.0596, 0.0208, 0.1233]\n",
    "                    [0.1308, 0.2070, 0.1570]\n",
    "                    [0.1330, 0.1983, 0.1493]\n",
    "                    [0.0273, 0.0719, 0.0409]\n",
    "                    [0.0833, 0.0270, 0.0108]\n",
    "                    [0.0079, 0.1265, 0.0870])   =   [0.4419, 0.6515, 0.5683]\n",
    "```\n"
   ],
   "id": "2c1302ec5c1766d0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T18:38:45.790902Z",
     "start_time": "2025-11-08T18:38:45.788558Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "807f3a29fde34fa1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e50bfb2549a5bd1e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
